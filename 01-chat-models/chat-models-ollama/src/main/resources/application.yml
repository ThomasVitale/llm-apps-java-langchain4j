spring:
  threads:
    virtual:
      enabled: true

langchain4j:
  ollama:
    chat:
      model: llama2
      options:
        temperature: 0.7
    client:
      log-requests: true
